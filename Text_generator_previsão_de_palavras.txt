# Vamos fazer um algorítmo que permite prever qual é a próxima palavra em uma
# sentença com base em um conjunto de sentenças que utilizamos para alimentar o
# modelo.
# Teremos como output a probabilidade de cada palavra ser a próxima.

# Vamos importar algumas bibliotecas que iremos utilizar

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import string
import numpy as np
import pandas as pd

# Vamos impor a base de dados

path = tf.keras.utils.get_file('songdata.csv','https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8')
print (path)
dataset = pd.read_csv(path, dtype=str)[:10]

# Vamos fazer o préprocessamento dos dados

def tokenize_corpus(corpus, num_words=-1):
  # Fit a Tokenizer on the corpus
  if num_words > -1:
    tokenizer = Tokenizer(num_words=num_words)
  else:
    tokenizer = Tokenizer()
  tokenizer.fit_on_texts(corpus)
  return tokenizer

def create_lyrics_corpus(dataset, field):
  # Remove all other punctuation
  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')
  # Make it lowercase
  dataset[field] = dataset[field].str.lower()
  # Make it one long string to split by line
  lyrics = dataset[field].str.cat()
  corpus = lyrics.split('\n')
  # Remove any trailing whitespace
  for l in range(len(corpus)):
    corpus[l] = corpus[l].rstrip()
  # Remove any empty lines
  corpus = [l for l in corpus if l != '']
  return corpus

# Read the dataset from csv - just first 10 songs for now
dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]
# Create the corpus using the 'text' column containing lyrics
corpus = create_lyrics_corpus(dataset, 'text')
# Tokenize the corpus
tokenizer = tokenize_corpus(corpus)

total_words = len(tokenizer.word_index) + 1

print(tokenizer.word_index)
print(total_words)

# Vamos criar sequências e "labels"

# After preprocessing, we next need to create sequences and labels. Creating the
# sequences themselves is similar to before with texts_to_sequences, but also
# including the use of N-Grams; creating the labels will now utilize those
# sequences as well as utilize one-hot encoding over all potential output words.

sequences = []
for line in corpus:
	token_list = tokenizer.texts_to_sequences([line])[0]
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		sequences.append(n_gram_sequence)

# Pad sequences for equal input length 
max_sequence_len = max([len(seq) for seq in sequences])
sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))

# Split sequences between the "input" sequence and "output" predicted word
input_sequences, labels = sequences[:,:-1], sequences[:,-1]
# One-hot encode the labels
one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)

# Check out how some of our data is being stored
# The Tokenizer has just a single index per word
print(tokenizer.word_index['know'])
print(tokenizer.word_index['feeling'])
# Input sequences will have multiple indexes
print(input_sequences[5])
print(input_sequences[6])
# And the one hot labels will be as long as the full spread of tokenized words
print(one_hot_labels[5])
print(one_hot_labels[6])

# Vamos treinar o modelo gerador de textos

# Building an RNN to train our text generation model will be very similar to the
# sentiment models you've built previously. The only real change necessary is to
# make sure to use Categorical instead of Binary Cross Entropy as the loss
# function - we could use Binary before since the sentiment was only 0 or 1, but
# now there are hundreds of categories.

# From there, we should also consider using more epochs than before, as text
# generation can take a little longer to converge than sentiment analysis, and we
# aren't working with all that much data yet. I'll set it at 200 epochs here
# since we're only use part of the dataset, and training will tail off quite a
# bit over that many epochs.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional

model = Sequential()
model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))
model.add(Bidirectional(LSTM(20)))
model.add(Dense(total_words, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)

# Vamos ver o gráfico de treinamento

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.show()

plot_graphs(history, 'accuracy')

# Por fim, vamos gerar os textos

# It's finally time to generate some new lyrics from the trained model, and see 
# what we get. To do so, we'll provide some "seed text", or an input sequence
# for the model to start with. We'll also decide just how long of an output
# sequence we want - this could essentially be infinite, as the input plus the
# previous output will be continuously fed in for a new output word (at least up
# to our max sequence length).

seed_text = "im feeling chills"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = np.argmax(model.predict(token_list), axis=-1)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word

print(seed_text)